{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5844f015",
   "metadata": {},
   "source": [
    "# Data Preparation Complete - Walmart Sales Forecast\n",
    "\n",
    "Notebook nÃ y thá»±c hiá»‡n toÃ n bá»™ cÃ¡c bÆ°á»›c chuáº©n bá»‹ dá»¯ liá»‡u theo file \"[CÃ¡c bÆ°á»›c lÃ m tham kháº£o tiáº¿p theo]\".\n",
    "\n",
    "## Cáº¥u trÃºc:\n",
    "\n",
    "**GIAI ÄOáº N 1: CHUáº¨N Bá»Š Dá»® LIá»†U**\n",
    "- 1.1. Táº¡o df_main_weekly\n",
    "- 1.2. Táº¡o df_events_daily  \n",
    "- 1.3. Táº¡o df_feature_calendar_weekly\n",
    "\n",
    "**GIAI ÄOáº N 2: FEATURE ENGINEERING**\n",
    "- 2.1. Merge & Kiá»ƒm tra\n",
    "- 2.2. Táº¡o Features \"Payday Pulse\"\n",
    "- 2.3. Táº¡o Features \"Holiday\"\n",
    "- 2.4. Táº¡o Features \"Lag/Rolling\"\n",
    "- 2.5. Táº¡o Features \"Interaction\"\n",
    "\n",
    "**GIAI ÄOáº N 3: LÆ¯U CÃC FILE OUTPUT**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532fb10e",
   "metadata": {},
   "source": [
    "## 0. Setup vÃ  Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cáº¥u hÃ¬nh pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1fc43",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Äá»‹nh nghÄ©a cÃ¡c helper functions (tá»« file `data_prep_utils.py`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_us_holidays(year):\n",
    "    \"\"\"TÃ­nh cÃ¡c ngÃ y lá»… Má»¹ cho má»™t nÄƒm\"\"\"\n",
    "    holidays = {}\n",
    "    \n",
    "    holidays[f'{year}-01-01'] = ('New Years Day', 1)\n",
    "    \n",
    "    # Super Bowl\n",
    "    super_bowl_dates = {2010: '2010-02-07', 2011: '2011-02-06', 2012: '2012-02-05'}\n",
    "    if year in super_bowl_dates:\n",
    "        holidays[super_bowl_dates[year]] = ('Super Bowl', 3)\n",
    "    \n",
    "    # Presidents Day\n",
    "    presidents_day_dates = {2010: '2010-02-15', 2011: '2011-02-21', 2012: '2012-02-20'}\n",
    "    if year in presidents_day_dates:\n",
    "        holidays[presidents_day_dates[year]] = ('Presidents Day', 1)\n",
    "    \n",
    "    # Memorial Day\n",
    "    memorial_day_dates = {2010: '2010-05-31', 2011: '2011-05-30', 2012: '2012-05-28'}\n",
    "    if year in memorial_day_dates:\n",
    "        holidays[memorial_day_dates[year]] = ('Memorial Day', 1)\n",
    "    \n",
    "    holidays[f'{year}-07-04'] = ('Independence Day', 1)\n",
    "    \n",
    "    # Labor Day\n",
    "    labor_day_dates = {2010: '2010-09-06', 2011: '2011-09-05', 2012: '2012-09-03'}\n",
    "    if year in labor_day_dates:\n",
    "        holidays[labor_day_dates[year]] = ('Labor Day', 3)\n",
    "    \n",
    "    # Thanksgiving\n",
    "    thanksgiving_dates = {2010: '2010-11-25', 2011: '2011-11-24', 2012: '2012-11-22'}\n",
    "    if year in thanksgiving_dates:\n",
    "        holidays[thanksgiving_dates[year]] = ('Thanksgiving', 5)\n",
    "    \n",
    "    holidays[f'{year}-12-25'] = ('Christmas', 5)\n",
    "    holidays[f'{year}-12-24'] = ('Christmas Eve', 3)\n",
    "    \n",
    "    return holidays\n",
    "\n",
    "\n",
    "def get_week_end_date(date):\n",
    "    \"\"\"TÃ­nh WeekEndDate (Thá»© SÃ¡u cuá»‘i tuáº§n) cho má»™t ngÃ y\"\"\"\n",
    "    weekday = date.weekday()  # 0=Monday, 4=Friday, 6=Sunday\n",
    "    if weekday == 4:  # Friday\n",
    "        return date\n",
    "    elif weekday == 5:  # Saturday\n",
    "        return date + timedelta(days=6)\n",
    "    elif weekday == 6:  # Sunday\n",
    "        return date + timedelta(days=5)\n",
    "    else:  # Monday-Thursday\n",
    "        return date + timedelta(days=4-weekday)\n",
    "\n",
    "\n",
    "def is_tax_refund_season(date):\n",
    "    \"\"\"Kiá»ƒm tra xem ngÃ y cÃ³ thuá»™c mÃ¹a hoÃ n thuáº¿ khÃ´ng (15/02 - 15/04)\"\"\"\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    if month == 2 and day >= 15:\n",
    "        return 1\n",
    "    elif month == 3:\n",
    "        return 1\n",
    "    elif month == 4 and day <= 15:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def calculate_weeks_since_payday(group):\n",
    "    \"\"\"TÃ­nh sá»‘ tuáº§n ká»ƒ tá»« payday gáº§n nháº¥t cho má»—i group (Store, Dept)\"\"\"\n",
    "    weeks_since = []\n",
    "    last_payday_week = None\n",
    "    \n",
    "    for idx, row in group.iterrows():\n",
    "        if row['is_semimonthly_payweek'] == 1:\n",
    "            last_payday_week = row['WeekEndDate']\n",
    "            weeks_since.append(0)\n",
    "        elif last_payday_week is not None:\n",
    "            weeks_diff = (row['WeekEndDate'] - last_payday_week).days // 7\n",
    "            weeks_since.append(weeks_diff)\n",
    "        else:\n",
    "            weeks_since.append(np.nan)\n",
    "    \n",
    "    return pd.Series(weeks_since, index=group.index)\n",
    "\n",
    "\n",
    "def piecewise_decay(weeks):\n",
    "    \"\"\"TÃ­nh giÃ¡ trá»‹ decay theo piecewise function\"\"\"\n",
    "    if weeks == 0:\n",
    "        return 1.0\n",
    "    elif weeks == 1:\n",
    "        return 0.7\n",
    "    elif weeks >= 2:\n",
    "        return 0.4\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def get_christmas_date(year):\n",
    "    \"\"\"Tráº£ vá» ngÃ y GiÃ¡ng sinh\"\"\"\n",
    "    return pd.Timestamp(f'{year}-12-25')\n",
    "\n",
    "\n",
    "def get_thanksgiving_date(year):\n",
    "    \"\"\"TÃ­nh ngÃ y Thanksgiving (Thá»© 5 thá»© 4 cá»§a thÃ¡ng 11)\"\"\"\n",
    "    nov_1 = pd.Timestamp(f'{year}-11-01')\n",
    "    first_thursday = nov_1 + timedelta(days=(3 - nov_1.weekday()) % 7)\n",
    "    if first_thursday.day > 7:\n",
    "        first_thursday = first_thursday - timedelta(days=7)\n",
    "    thanksgiving = first_thursday + timedelta(days=21)\n",
    "    return thanksgiving\n",
    "\n",
    "\n",
    "def calculate_weeks_until_holiday(date, holiday_func):\n",
    "    \"\"\"TÃ­nh sá»‘ tuáº§n cho Ä‘áº¿n lá»… tiáº¿p theo\"\"\"\n",
    "    year = date.year\n",
    "    holiday_date = holiday_func(year)\n",
    "    \n",
    "    # Náº¿u lá»… Ä‘Ã£ qua trong nÄƒm nÃ y, tÃ­nh lá»… nÄƒm sau\n",
    "    if date > holiday_date:\n",
    "        holiday_date = holiday_func(year + 1)\n",
    "    \n",
    "    weeks_diff = (holiday_date - date).days // 7\n",
    "    return weeks_diff\n",
    "\n",
    "print(\"âœ… Helper functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afff556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n\n",
    "DATA_PATH = '../data/'\n",
    "PROCESSED_PATH = '../data/processed/'\n",
    "\n",
    "print(f\"ğŸ“ Data path: {DATA_PATH}\")\n",
    "print(f\"ğŸ“ Processed path: {PROCESSED_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f29b80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# GIAI ÄOáº N 1: CHUáº¨N Bá»Š Dá»® LIá»†U\n",
    "\n",
    "## 1.1. Táº¡o df_main_weekly\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "- Merge 3 files: train.csv, stores.csv, features.csv\n",
    "- Xá»­ lÃ½ MarkDowns (fillna, táº¡o features)\n",
    "- Xá»­ lÃ½ Weekly_Sales Ã¢m (returns)\n",
    "- Validation dá»¯ liá»‡u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e968282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cÃ¡c datasets\n",
    "print(\"ğŸ”„ Loading datasets...\")\n",
    "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "stores_df = pd.read_csv(DATA_PATH + 'stores.csv')\n",
    "features_df = pd.read_csv(DATA_PATH + 'features.csv')\n",
    "\n",
    "print(f\"ğŸ“ˆ Train data shape: {train_df.shape}\")\n",
    "print(f\"ğŸª Stores data shape: {stores_df.shape}\")\n",
    "print(f\"ğŸŒ¡ï¸ Features data shape: {features_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3345d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 3 files\n",
    "print(\"ğŸ”„ Merging datasets...\")\n",
    "df_main = pd.merge(train_df, stores_df, on='Store', how='left')\n",
    "df_main = pd.merge(df_main, features_df, on=['Store', 'Date'], how='left', suffixes=('', '_features'))\n",
    "\n",
    "# Xá»­ lÃ½ duplicate IsHoliday columns\n",
    "if 'IsHoliday_features' in df_main.columns:\n",
    "    df_main = df_main.drop(columns=['IsHoliday_features'])\n",
    "\n",
    "print(f\"âœ… Merged data shape: {df_main.shape}\")\n",
    "print(f\"\\nSample data:\")\n",
    "df_main.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyá»ƒn Ä‘á»•i Date sang datetime vÃ  Ä‘á»•i tÃªn thÃ nh WeekEndDate\n",
    "df_main['Date'] = pd.to_datetime(df_main['Date'])\n",
    "df_main = df_main.rename(columns={'Date': 'WeekEndDate'})\n",
    "\n",
    "print(f\"ğŸ“… Time range: {df_main['WeekEndDate'].min()} to {df_main['WeekEndDate'].max()}\")\n",
    "\n",
    "# Kiá»ƒm tra WeekEndDate cÃ³ pháº£i lÃ  Thá»© SÃ¡u khÃ´ng\n",
    "df_main['WeekDay'] = df_main['WeekEndDate'].dt.day_name()\n",
    "print(f\"\\nWeekday distribution:\")\n",
    "print(df_main['WeekDay'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xá»­ lÃ½ MarkDowns\n",
    "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
    "\n",
    "# Fill NA vá»›i 0\n",
    "for col in markdown_cols:\n",
    "    df_main[col] = df_main[col].fillna(0)\n",
    "\n",
    "# Táº¡o md_missing_any: = 1 náº¿u cáº£ 5 cá»™t Ä‘á»u lÃ  0\n",
    "df_main['md_missing_any'] = ((df_main[markdown_cols] == 0).all(axis=1)).astype(int)\n",
    "\n",
    "# Táº¡o md_sum: Tá»•ng giÃ¡ trá»‹ cá»§a 5 cá»™t MarkDown\n",
    "df_main['md_sum'] = df_main[markdown_cols].sum(axis=1)\n",
    "\n",
    "print(f\"ğŸ“Š MarkDowns missing (all zeros): {df_main['md_missing_any'].sum()} records\")\n",
    "print(f\"\\nMarkDowns sum statistics:\")\n",
    "print(df_main['md_sum'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xá»­ lÃ½ Weekly_Sales Ã¢m\n",
    "# Táº¡o returns_flag vÃ  returns_abs trÆ°á»›c khi clip\n",
    "df_main['returns_flag'] = (df_main['Weekly_Sales'] < 0).astype(int)\n",
    "df_main['returns_abs'] = df_main['Weekly_Sales'].apply(lambda x: abs(x) if x < 0 else 0)\n",
    "\n",
    "print(f\"ğŸ“Š Negative sales processed: {df_main['returns_flag'].sum()} records\")\n",
    "print(f\"ğŸ“Š Total returns value: ${df_main['returns_abs'].sum():,.2f}\")\n",
    "\n",
    "# Clip Weekly_Sales vá» >= 0\n",
    "df_main['Weekly_Sales'] = df_main['Weekly_Sales'].clip(lower=0)\n",
    "\n",
    "print(f\"âœ… All Weekly_Sales are now >= 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "print(\"ğŸ” Validating data quality...\")\n",
    "\n",
    "# Kiá»ƒm tra WeekEndDate khÃ´ng bá»‹ NA\n",
    "assert df_main['WeekEndDate'].notna().all(), \"WeekEndDate has NA!\"\n",
    "\n",
    "# Kiá»ƒm tra Store vÃ  Dept khÃ´ng bá»‹ NA\n",
    "assert df_main['Store'].notna().all(), \"Store has NA!\"\n",
    "assert df_main['Dept'].notna().all(), \"Dept has NA!\"\n",
    "\n",
    "# Kiá»ƒm tra Weekly_Sales (sau clip) khÃ´ng Ã¢m\n",
    "assert (df_main['Weekly_Sales'] >= 0).all(), \"Weekly_Sales still negative!\"\n",
    "\n",
    "print(\"âœ… All validations passed!\")\n",
    "\n",
    "# LÆ°u df_main_weekly\n",
    "df_main_weekly = df_main.copy()\n",
    "print(f\"\\nğŸ“Š df_main_weekly created! Shape: {df_main_weekly.shape}\")\n",
    "print(f\"ğŸ“Š Columns: {len(df_main_weekly.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928c612",
   "metadata": {},
   "source": [
    "## 1.2. Táº¡o df_events_daily\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "- Táº¡o lá»‹ch daily tá»« min_date Ä‘áº¿n max_date\n",
    "- ThÃªm features Payday (SNAP, semimonthly, tax refund)\n",
    "- Táº¡o lá»‹ch Holiday events cho Má»¹ (2010-2012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ada3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o lá»‹ch daily tá»« min_date Ä‘áº¿n max_date\n",
    "min_date = df_main_weekly['WeekEndDate'].min()\n",
    "max_date = df_main_weekly['WeekEndDate'].max()\n",
    "\n",
    "date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "df_events_daily = pd.DataFrame({'Date': date_range})\n",
    "\n",
    "print(f\"ğŸ“… Date range: {min_date.date()} to {max_date.date()}\")\n",
    "print(f\"ğŸ“… Total days: {len(df_events_daily)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ThÃªm features Payday\n",
    "# is_snap_window_1: ngÃ y 1-10\n",
    "df_events_daily['is_snap_window_1'] = (df_events_daily['Date'].dt.day <= 10).astype(int)\n",
    "\n",
    "# is_snap_window_2: ngÃ y 11-20\n",
    "df_events_daily['is_snap_window_2'] = ((df_events_daily['Date'].dt.day >= 11) & \n",
    "                                       (df_events_daily['Date'].dt.day <= 20)).astype(int)\n",
    "\n",
    "# is_semimonthly_payday: ngÃ y 15 hoáº·c cuá»‘i thÃ¡ng\n",
    "df_events_daily['is_semimonthly_payday'] = ((df_events_daily['Date'].dt.day == 15) | \n",
    "                                           (df_events_daily['Date'].dt.is_month_end)).astype(int)\n",
    "\n",
    "# is_tax_refund_season: 15/02 - 15/04 hÃ ng nÄƒm\n",
    "df_events_daily['is_tax_refund_season'] = df_events_daily['Date'].apply(is_tax_refund_season)\n",
    "\n",
    "print(\"âœ… Payday features added!\")\n",
    "print(f\"   SNAP window 1 days: {df_events_daily['is_snap_window_1'].sum()}\")\n",
    "print(f\"   SNAP window 2 days: {df_events_daily['is_snap_window_2'].sum()}\")\n",
    "print(f\"   Semimonthly payday days: {df_events_daily['is_semimonthly_payday'].sum()}\")\n",
    "print(f\"   Tax refund season days: {df_events_daily['is_tax_refund_season'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o lá»‹ch Holiday events cho Má»¹ (2010-2012)\n",
    "all_holidays = {}\n",
    "for year in [2010, 2011, 2012]:\n",
    "    all_holidays.update(get_us_holidays(year))\n",
    "\n",
    "# Map vÃ o df_events_daily\n",
    "df_events_daily['HolidayName'] = df_events_daily['Date'].dt.strftime('%Y-%m-%d').map(\n",
    "    lambda x: all_holidays.get(x, ('', 0))[0] if x in all_holidays else ''\n",
    ")\n",
    "df_events_daily['holiday_impact'] = df_events_daily['Date'].dt.strftime('%Y-%m-%d').map(\n",
    "    lambda x: all_holidays.get(x, ('', 0))[1] if x in all_holidays else 0\n",
    ")\n",
    "\n",
    "print(\"âœ… Holiday events added!\")\n",
    "print(f\"   Total holidays: {(df_events_daily['HolidayName'] != '').sum()}\")\n",
    "print(f\"\\nHoliday distribution:\")\n",
    "print(df_events_daily[df_events_daily['HolidayName'] != '']['HolidayName'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem sample df_events_daily\n",
    "print(\"ğŸ“Š df_events_daily shape:\", df_events_daily.shape)\n",
    "print(\"\\nSample data:\")\n",
    "df_events_daily.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f267b",
   "metadata": {},
   "source": [
    "## 1.3. Táº¡o df_feature_calendar_weekly\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "- ThÃªm WeekEndDate vÃ o df_events_daily\n",
    "- Groupby WeekEndDate vÃ  aggregate cÃ¡c features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82befce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ThÃªm WeekEndDate vÃ o df_events_daily\n",
    "df_events_daily['WeekEndDate'] = df_events_daily['Date'].apply(get_week_end_date)\n",
    "\n",
    "print(\"âœ… WeekEndDate added to df_events_daily!\")\n",
    "print(f\"\\nSample WeekEndDate mapping:\")\n",
    "print(df_events_daily[['Date', 'WeekEndDate']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby WeekEndDate vÃ  aggregate\n",
    "df_feature_calendar_weekly = df_events_daily.groupby('WeekEndDate').agg({\n",
    "    'is_snap_window_1': lambda x: 1 if x.sum() > 0 else 0,\n",
    "    'is_snap_window_2': lambda x: 1 if x.sum() > 0 else 0,\n",
    "    'is_semimonthly_payday': lambda x: 1 if x.sum() > 0 else 0,\n",
    "    'is_tax_refund_season': lambda x: 1 if x.sum() > 0 else 0,\n",
    "    'holiday_impact': 'max',\n",
    "    'HolidayName': lambda x: x[x != ''].iloc[0] if (x != '').any() else ''\n",
    "}).reset_index()\n",
    "\n",
    "# Äá»•i tÃªn cá»™t\n",
    "df_feature_calendar_weekly = df_feature_calendar_weekly.rename(columns={\n",
    "    'is_snap_window_1': 'is_snap_window_1_week',\n",
    "    'is_snap_window_2': 'is_snap_window_2_week',\n",
    "    'is_semimonthly_payday': 'is_semimonthly_payweek',\n",
    "    'is_tax_refund_season': 'is_tax_refund_season_week',\n",
    "    'holiday_impact': 'holiday_impact_week',\n",
    "    'HolidayName': 'holiday_name_week'\n",
    "})\n",
    "\n",
    "print(f\"âœ… df_feature_calendar_weekly created! Shape: {df_feature_calendar_weekly.shape}\")\n",
    "print(f\"\\nSample data:\")\n",
    "df_feature_calendar_weekly.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae749465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem statistics cá»§a weekly features\n",
    "print(\"ğŸ“Š Weekly features statistics:\")\n",
    "print(f\"\\n   SNAP window 1 weeks: {df_feature_calendar_weekly['is_snap_window_1_week'].sum()}\")\n",
    "print(f\"   SNAP window 2 weeks: {df_feature_calendar_weekly['is_snap_window_2_week'].sum()}\")\n",
    "print(f\"   Semimonthly payweeks: {df_feature_calendar_weekly['is_semimonthly_payweek'].sum()}\")\n",
    "print(f\"   Tax refund season weeks: {df_feature_calendar_weekly['is_tax_refund_season_week'].sum()}\")\n",
    "print(f\"   Weeks with holidays: {(df_feature_calendar_weekly['holiday_name_week'] != '').sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2589104",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# GIAI ÄOáº N 2: FEATURE ENGINEERING\n",
    "\n",
    "## 2.1. Merge & Kiá»ƒm tra\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "- Merge df_main_weekly vá»›i df_feature_calendar_weekly\n",
    "- Sanity check: kiá»ƒm tra uniqueness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f24ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_main_weekly vá»›i df_feature_calendar_weekly\n",
    "df_final = pd.merge(df_main_weekly, df_feature_calendar_weekly, on='WeekEndDate', how='left')\n",
    "\n",
    "# Fillna cho cÃ¡c cá»™t má»›i\n",
    "fill_cols = ['is_snap_window_1_week', 'is_snap_window_2_week', 'is_semimonthly_payweek', \n",
    "             'is_tax_refund_season_week', 'holiday_impact_week']\n",
    "for col in fill_cols:\n",
    "    df_final[col] = df_final[col].fillna(0)\n",
    "\n",
    "df_final['holiday_name_week'] = df_final['holiday_name_week'].fillna('')\n",
    "\n",
    "print(f\"âœ… Merge completed! Shape: {df_final.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a5e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Kiá»ƒm tra uniqueness cá»§a (Store, Dept, WeekEndDate)\n",
    "print(\"ğŸ” Sanity check...\")\n",
    "duplicates = df_final.groupby(['Store', 'Dept', 'WeekEndDate']).size()\n",
    "if (duplicates > 1).any():\n",
    "    print(f\"âš ï¸ Warning: Found {((duplicates > 1).sum())} duplicate (Store, Dept, WeekEndDate) combinations!\")\n",
    "    print(duplicates[duplicates > 1].head())\n",
    "else:\n",
    "    print(\"âœ… No duplicates found! Each (Store, Dept, WeekEndDate) is unique.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6219d456",
   "metadata": {},
   "source": [
    "## 2.2. Táº¡o Features \"Payday Pulse\"\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "- Táº¡o weeks_since_payday_15_eom\n",
    "- Táº¡o payday_decay_exp vÃ  payday_decay_piecewise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sáº¯p xáº¿p theo Store, Dept, WeekEndDate Ä‘á»ƒ tÃ­nh lag\n",
    "df_final = df_final.sort_values(['Store', 'Dept', 'WeekEndDate']).reset_index(drop=True)\n",
    "\n",
    "# weeks_since_payday_15_eom: Äáº¿m sá»‘ tuáº§n ká»ƒ tá»« is_semimonthly_payweek gáº§n nháº¥t\n",
    "df_final['weeks_since_payday_15_eom'] = df_final.groupby(['Store', 'Dept']).apply(\n",
    "    calculate_weeks_since_payday, include_groups=False\n",
    ").reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# Fillna vá»›i giÃ¡ trá»‹ lá»›n (náº¿u chÆ°a cÃ³ payday nÃ o)\n",
    "df_final['weeks_since_payday_15_eom'] = df_final['weeks_since_payday_15_eom'].fillna(999)\n",
    "\n",
    "print(\"âœ… weeks_since_payday_15_eom created!\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(df_final['weeks_since_payday_15_eom'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o features decay\n",
    "# CÃ¡ch 1: Exponential decay\n",
    "df_final['payday_decay_exp'] = np.exp(-0.25 * df_final['weeks_since_payday_15_eom'])\n",
    "\n",
    "# CÃ¡ch 2: Piecewise decay\n",
    "df_final['payday_decay_piecewise'] = df_final['weeks_since_payday_15_eom'].apply(piecewise_decay)\n",
    "\n",
    "print(\"âœ… Payday decay features created!\")\n",
    "print(f\"\\nPayday decay statistics:\")\n",
    "print(df_final[['payday_decay_exp', 'payday_decay_piecewise']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa88e87",
   "metadata": {},
   "source": [
    "## 2.3. Táº¡o Features \"Holiday\"\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "- Táº¡o weeks_until_christmas vÃ  weeks_until_thanksgiving\n",
    "- Táº¡o is_pre_christmas_window_week vÃ  is_pre_thanksgiving_window_week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o features Holiday countdown\n",
    "df_final['weeks_until_christmas'] = df_final['WeekEndDate'].apply(\n",
    "    lambda x: calculate_weeks_until_holiday(x, get_christmas_date)\n",
    ")\n",
    "df_final['weeks_until_thanksgiving'] = df_final['WeekEndDate'].apply(\n",
    "    lambda x: calculate_weeks_until_holiday(x, get_thanksgiving_date)\n",
    ")\n",
    "\n",
    "print(\"âœ… Holiday countdown features created!\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"   weeks_until_christmas - min: {df_final['weeks_until_christmas'].min()}, max: {df_final['weeks_until_christmas'].max()}\")\n",
    "print(f\"   weeks_until_thanksgiving - min: {df_final['weeks_until_thanksgiving'].min()}, max: {df_final['weeks_until_thanksgiving'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o features Holiday window\n",
    "df_final['is_pre_christmas_window_week'] = (df_final['weeks_until_christmas'] <= 3).astype(int)\n",
    "df_final['is_pre_thanksgiving_window_week'] = (df_final['weeks_until_thanksgiving'] <= 2).astype(int)\n",
    "\n",
    "print(\"âœ… Holiday window features created!\")\n",
    "print(f\"   Pre-Christmas weeks: {df_final['is_pre_christmas_window_week'].sum()}\")\n",
    "print(f\"   Pre-Thanksgiving weeks: {df_final['is_pre_thanksgiving_window_week'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c42e0",
   "metadata": {},
   "source": [
    "## 2.4. Táº¡o Features \"Lag/Rolling\"\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "- Táº¡o lag features (t-52, t-1, t-2, t-4)\n",
    "- Táº¡o rolling statistics (mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c096189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o features Lag\n",
    "df_final = df_final.sort_values(['Store', 'Dept', 'WeekEndDate']).reset_index(drop=True)\n",
    "\n",
    "# lag_sales_t_52: Feature \"nÄƒm ngoÃ¡i\" (52 tuáº§n trÆ°á»›c)\n",
    "df_final['lag_sales_t_52'] = df_final.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(52)\n",
    "\n",
    "# lag_sales_t_1, lag_sales_t_2, lag_sales_t_4: Lag ngáº¯n háº¡n\n",
    "df_final['lag_sales_t_1'] = df_final.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
    "df_final['lag_sales_t_2'] = df_final.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2)\n",
    "df_final['lag_sales_t_4'] = df_final.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(4)\n",
    "\n",
    "print(\"âœ… Lag features created!\")\n",
    "print(f\"\\nLag features missing values:\")\n",
    "print(df_final[['lag_sales_t_52', 'lag_sales_t_1', 'lag_sales_t_2', 'lag_sales_t_4']].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049463c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o features Rolling\n",
    "# rolling_mean_sales_4_weeks: Trung bÃ¬nh 4 tuáº§n gáº§n nháº¥t (shift(1) Ä‘á»ƒ trÃ¡nh leakage)\n",
    "df_final['rolling_mean_sales_4_weeks'] = df_final.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=4, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# rolling_std_sales_4_weeks: Äá»™ lá»‡ch chuáº©n 4 tuáº§n\n",
    "df_final['rolling_std_sales_4_weeks'] = df_final.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=4, min_periods=1).std()\n",
    ")\n",
    "\n",
    "print(\"âœ… Rolling features created!\")\n",
    "print(f\"\\nRolling features statistics:\")\n",
    "print(df_final[['rolling_mean_sales_4_weeks', 'rolling_std_sales_4_weeks']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f20a1",
   "metadata": {},
   "source": [
    "## 2.5. Táº¡o Features \"Interaction\"\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "- Táº¡o interact_snap_x_type_c\n",
    "- Táº¡o interact_holiday_x_impact\n",
    "- Táº¡o interact_tax_x_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535108eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o features Interaction\n",
    "# interact_snap_x_type_c: SNAP x Store Type C\n",
    "df_final['interact_snap_x_type_c'] = df_final['is_snap_window_1_week'] * (df_final['Type'] == 'C').astype(int)\n",
    "\n",
    "# interact_holiday_x_impact: Pre-Christmas window x holiday impact\n",
    "df_final['interact_holiday_x_impact'] = df_final['is_pre_christmas_window_week'] * df_final['holiday_impact_week']\n",
    "\n",
    "# interact_tax_x_temp: Tax refund season x Temperature\n",
    "df_final['interact_tax_x_temp'] = df_final['is_tax_refund_season_week'] * df_final['Temperature']\n",
    "\n",
    "print(\"âœ… Interaction features created!\")\n",
    "print(f\"\\nInteraction features statistics:\")\n",
    "print(df_final[['interact_snap_x_type_c', 'interact_holiday_x_impact', 'interact_tax_x_temp']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829fb9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# GIAI ÄOáº N 3: LÆ¯U CÃC FILE OUTPUT\n",
    "\n",
    "**Má»¥c tiÃªu:**\n",
    "- LÆ°u df_main_weekly.csv\n",
    "- LÆ°u df_events_daily.csv\n",
    "- LÆ°u df_feature_calendar_weekly.csv\n",
    "- LÆ°u df_final_for_model.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594767a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÆ°u cÃ¡c file output\n",
    "print(\"ğŸ’¾ Saving output files...\")\n",
    "\n",
    "# Táº¡o thÆ° má»¥c processed náº¿u chÆ°a cÃ³\n",
    "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "# LÆ°u df_main_weekly\n",
    "df_main_weekly.to_csv(PROCESSED_PATH + 'df_main_weekly.csv', index=False)\n",
    "print(f\"âœ… Saved: df_main_weekly.csv ({df_main_weekly.shape})\")\n",
    "\n",
    "# LÆ°u df_events_daily\n",
    "df_events_daily.to_csv(PROCESSED_PATH + 'df_events_daily.csv', index=False)\n",
    "print(f\"âœ… Saved: df_events_daily.csv ({df_events_daily.shape})\")\n",
    "\n",
    "# LÆ°u df_feature_calendar_weekly\n",
    "df_feature_calendar_weekly.to_csv(PROCESSED_PATH + 'df_feature_calendar_weekly.csv', index=False)\n",
    "print(f\"âœ… Saved: df_feature_calendar_weekly.csv ({df_feature_calendar_weekly.shape})\")\n",
    "\n",
    "# LÆ°u df_final_for_model\n",
    "df_final.to_csv(PROCESSED_PATH + 'df_final_for_model.csv', index=False)\n",
    "print(f\"âœ… Saved: df_final_for_model.csv ({df_final.shape})\")\n",
    "\n",
    "print(\"\\nğŸ‰ All files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38cd45d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# TÃ“M Táº®T Káº¾T QUáº¢\n",
    "\n",
    "## Final Dataset Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c0814",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“Š Final dataset shape: {df_final.shape}\")\n",
    "print(f\"ğŸ“Š Total columns: {len(df_final.columns)}\")\n",
    "print(f\"ğŸ“… Time range: {df_final['WeekEndDate'].min()} to {df_final['WeekEndDate'].max()}\")\n",
    "print(f\"ğŸª Stores: {df_final['Store'].nunique()}, Departments: {df_final['Dept'].nunique()}\")\n",
    "\n",
    "print(f\"\\nğŸ’° Weekly Sales statistics:\")\n",
    "print(df_final['Weekly_Sales'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fa552",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nğŸ“‹ All columns ({len(df_final.columns)}):\")\n",
    "for i, col in enumerate(df_final.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… DATA PREPARATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84613052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem sample cá»§a df_final\n",
    "print(\"Sample of final dataset:\")\n",
    "df_final.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Walmart)",
   "language": "python",
   "name": "walmart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
